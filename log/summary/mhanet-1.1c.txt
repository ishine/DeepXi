Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp (InputLayer)                [(None, None, 257)]  0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, None, 256)    65792       inp[0][0]                        
__________________________________________________________________________________________________
layer_normalization (LayerNorma (None, None, 256)    512         conv1d[0][0]                     
__________________________________________________________________________________________________
re_lu (ReLU)                    (None, None, 256)    0           layer_normalization[0][0]        
__________________________________________________________________________________________________
tf_op_layer_Shape (TensorFlowOp [(3,)]               0           re_lu[0][0]                      
__________________________________________________________________________________________________
tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          
__________________________________________________________________________________________________
tf_op_layer_Shape_1 (TensorFlow [(3,)]               0           re_lu[0][0]                      
__________________________________________________________________________________________________
tf_op_layer_Range (TensorFlowOp [(None,)]            0           tf_op_layer_strided_slice[0][0]  
__________________________________________________________________________________________________
tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        
__________________________________________________________________________________________________
tf_op_layer_Tile/input (TensorF [(1, None)]          0           tf_op_layer_Range[0][0]          
__________________________________________________________________________________________________
tf_op_layer_Tile/multiples (Ten [(2,)]               0           tf_op_layer_strided_slice_1[0][0]
__________________________________________________________________________________________________
tf_op_layer_Tile (TensorFlowOpL [(None, None)]       0           tf_op_layer_Tile/input[0][0]     
                                                                 tf_op_layer_Tile/multiples[0][0] 
__________________________________________________________________________________________________
embedding (Embedding)           (None, None, 256)    524288      tf_op_layer_Tile[0][0]           
__________________________________________________________________________________________________
add (Add)                       (None, None, 256)    0           re_lu[0][0]                      
                                                                 embedding[0][0]                  
__________________________________________________________________________________________________
attention_mask_v2 (AttentionMas (None, 1, None, None 0           inp[0][0]                        
__________________________________________________________________________________________________
multi_head_attention (MultiHead (None, None, 256)    262144      add[0][0]                        
                                                                 add[0][0]                        
                                                                 add[0][0]                        
                                                                 attention_mask_v2[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, None, 256)    0           add[0][0]                        
                                                                 multi_head_attention[0][0]       
__________________________________________________________________________________________________
layer_normalization_1 (LayerNor (None, None, 256)    512         add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, None, 1024)   263168      layer_normalization_1[0][0]      
__________________________________________________________________________________________________
re_lu_1 (ReLU)                  (None, None, 1024)   0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, None, 256)    262400      re_lu_1[0][0]                    
__________________________________________________________________________________________________
add_2 (Add)                     (None, None, 256)    0           layer_normalization_1[0][0]      
                                                                 conv1d_2[0][0]                   
__________________________________________________________________________________________________
layer_normalization_2 (LayerNor (None, None, 256)    512         add_2[0][0]                      
__________________________________________________________________________________________________
multi_head_attention_1 (MultiHe (None, None, 256)    262144      layer_normalization_2[0][0]      
                                                                 layer_normalization_2[0][0]      
                                                                 layer_normalization_2[0][0]      
                                                                 attention_mask_v2[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, None, 256)    0           layer_normalization_2[0][0]      
                                                                 multi_head_attention_1[0][0]     
__________________________________________________________________________________________________
layer_normalization_3 (LayerNor (None, None, 256)    512         add_3[0][0]                      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, None, 1024)   263168      layer_normalization_3[0][0]      
__________________________________________________________________________________________________
re_lu_2 (ReLU)                  (None, None, 1024)   0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, None, 256)    262400      re_lu_2[0][0]                    
__________________________________________________________________________________________________
add_4 (Add)                     (None, None, 256)    0           layer_normalization_3[0][0]      
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
layer_normalization_4 (LayerNor (None, None, 256)    512         add_4[0][0]                      
__________________________________________________________________________________________________
multi_head_attention_2 (MultiHe (None, None, 256)    262144      layer_normalization_4[0][0]      
                                                                 layer_normalization_4[0][0]      
                                                                 layer_normalization_4[0][0]      
                                                                 attention_mask_v2[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, None, 256)    0           layer_normalization_4[0][0]      
                                                                 multi_head_attention_2[0][0]     
__________________________________________________________________________________________________
layer_normalization_5 (LayerNor (None, None, 256)    512         add_5[0][0]                      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, None, 1024)   263168      layer_normalization_5[0][0]      
__________________________________________________________________________________________________
re_lu_3 (ReLU)                  (None, None, 1024)   0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, None, 256)    262400      re_lu_3[0][0]                    
__________________________________________________________________________________________________
add_6 (Add)                     (None, None, 256)    0           layer_normalization_5[0][0]      
                                                                 conv1d_6[0][0]                   
__________________________________________________________________________________________________
layer_normalization_6 (LayerNor (None, None, 256)    512         add_6[0][0]                      
__________________________________________________________________________________________________
multi_head_attention_3 (MultiHe (None, None, 256)    262144      layer_normalization_6[0][0]      
                                                                 layer_normalization_6[0][0]      
                                                                 layer_normalization_6[0][0]      
                                                                 attention_mask_v2[0][0]          
__________________________________________________________________________________________________
add_7 (Add)                     (None, None, 256)    0           layer_normalization_6[0][0]      
                                                                 multi_head_attention_3[0][0]     
__________________________________________________________________________________________________
layer_normalization_7 (LayerNor (None, None, 256)    512         add_7[0][0]                      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, None, 1024)   263168      layer_normalization_7[0][0]      
__________________________________________________________________________________________________
re_lu_4 (ReLU)                  (None, None, 1024)   0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, None, 256)    262400      re_lu_4[0][0]                    
__________________________________________________________________________________________________
add_8 (Add)                     (None, None, 256)    0           layer_normalization_7[0][0]      
                                                                 conv1d_8[0][0]                   
__________________________________________________________________________________________________
layer_normalization_8 (LayerNor (None, None, 256)    512         add_8[0][0]                      
__________________________________________________________________________________________________
multi_head_attention_4 (MultiHe (None, None, 256)    262144      layer_normalization_8[0][0]      
                                                                 layer_normalization_8[0][0]      
                                                                 layer_normalization_8[0][0]      
                                                                 attention_mask_v2[0][0]          
__________________________________________________________________________________________________
add_9 (Add)                     (None, None, 256)    0           layer_normalization_8[0][0]      
                                                                 multi_head_attention_4[0][0]     
__________________________________________________________________________________________________
layer_normalization_9 (LayerNor (None, None, 256)    512         add_9[0][0]                      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, None, 1024)   263168      layer_normalization_9[0][0]      
__________________________________________________________________________________________________
re_lu_5 (ReLU)                  (None, None, 1024)   0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, None, 256)    262400      re_lu_5[0][0]                    
__________________________________________________________________________________________________
add_10 (Add)                    (None, None, 256)    0           layer_normalization_9[0][0]      
                                                                 conv1d_10[0][0]                  
__________________________________________________________________________________________________
layer_normalization_10 (LayerNo (None, None, 256)    512         add_10[0][0]                     
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, None, 257)    66049       layer_normalization_10[0][0]     
__________________________________________________________________________________________________
activation (Activation)         (None, None, 257)    0           conv1d_11[0][0]                  
==================================================================================================
Total params: 4,600,321
Trainable params: 4,600,321
Non-trainable params: 0
__________________________________________________________________________________________________
